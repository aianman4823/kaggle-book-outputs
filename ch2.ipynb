{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "11d8628a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_squared_log_error, mean_absolute_error, r2_score, confusion_matrix,accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ecbf106",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [1.0, 1.5, 2.0, 1.2, 1.8]\n",
    "y_pred = [0.8, 1.5, 1.8, 1.3, 3.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "512b6c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5531726674375732\n"
     ]
    }
   ],
   "source": [
    "# RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_true, y_pred)) \n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e4b5daf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17032547044118188"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RMSLE\n",
    "rmsle = np.sqrt(mean_squared_log_error(y_true, y_pred))\n",
    "rmsle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4b0ee336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33999999999999997"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MAE\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c4e7a9f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.2499999999999996"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 決定係数は最大で1\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "098d0cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 1]\n",
      " [2 2]]\n",
      "[[2 1]\n",
      " [2 3]]\n"
     ]
    }
   ],
   "source": [
    "# 混合行列\n",
    "# 0, 1で表される二値分類の真の値と予測値\n",
    "y_true_b = [1, 0, 1, 1, 0, 1, 1, 0]\n",
    "y_pred_b = [0, 0, 1, 1, 0, 0, 1, 1]\n",
    "\n",
    "# andではなく&を使わないといけない。なぜならこれは0, 1の論理積だから。普通の論理演算ではない。\n",
    "tp = np.sum((np.array(y_true_b) == 1) & (np.array(y_pred_b) == 1))\n",
    "tn = np.sum((np.array(y_true_b) == 0) & (np.array(y_pred_b) == 0))\n",
    "fp = np.sum((np.array(y_true_b) == 0) & (np.array(y_pred_b) == 1))\n",
    "fn = np.sum((np.array(y_true_b) == 1) & (np.array(y_pred_b) == 0))\n",
    "# T正, F正\n",
    "# F負, T負\n",
    "confusion_matrix1 = np.array([[tp, fp],[fn, tn]])\n",
    "print(confusion_matrix1)\n",
    "\n",
    "# scikit-learnのmetricsモジュールのconfusion_matrixでも作成できるが、\n",
    "# 混合行列の要素の配置が違うので注意\n",
    "confusion_matrix2 = confusion_matrix(y_true_b, y_pred_b)\n",
    "# 　　負正\n",
    "# F\n",
    "# T\n",
    "print(confusion_matrix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8eaab9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "?confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "77191ca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.625"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracyとerror rate\n",
    "# 上のバイナリの例を利用\n",
    "accuracy = np.sum(confusion_matrix2[1])/np.sum(confusion_matrix2)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cf5d3d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error rate:  0.375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.625"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scikit-learn.matricsのaccuracy_scoreを利用した場合\n",
    "accuracy2 = accuracy_score(y_true_b, y_pred_b)\n",
    "print('error rate: ', 1 - accuracy2)\n",
    "accuracy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "280f8724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision(適合率)とrecall(再現率)\n",
    "# from sklearn.metrics import precision_score, recall_score\n",
    "# precision = TP / (TP + FP)\n",
    "# recall = TP / (TP + FN)\n",
    "# これらは互いにトレードオフなので、同時に考慮する必要がある"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ba705823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f値:  0.6666666666666665\n",
      "f2値:  0.625\n"
     ]
    }
   ],
   "source": [
    "# scikit-learnのmetricsモジュールのprecision_score, recall_scoreを用いてトレードオフを考慮した計算ができる\n",
    "# F1-scoreとFβ-score(precisionとrecallの調和平均で計算される指標)\n",
    "from sklearn.metrics import f1_score, fbeta_score\n",
    "print('f値: ',f1_score(y_true_b, y_pred_b))\n",
    "# βを2とする\n",
    "print('f2値: ',fbeta_score(y_true_b, y_pred_b, beta=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "694c3248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCC:  0.2581988897471611\n"
     ]
    }
   ],
   "source": [
    "# MCC(Matthews Correlation Coefficient)\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "print('MCC: ', matthews_corrcoef(y_true_b, y_pred_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c0818c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "?matthews_corrcoef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4cdc2196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7135581778200728\n"
     ]
    }
   ],
   "source": [
    "# 二値分類における評価指標(正例である確率を予測値とする場合)\n",
    "# logloss(cross-entropy)\n",
    "from sklearn.metrics import log_loss\n",
    "y_true_c = [1, 0, 1, 1, 0, 1]\n",
    "y_prob_c = [0.1, 0.2, 0.8, 0.8, 0.1, 0.3]\n",
    "logloss = log_loss(y_true_c, y_prob_c)\n",
    "print(logloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "06721ce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8125"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AUC\n",
    "# また、評価指標がGini係数の時はGini = 2 * AUC - 1で線形関係より、評価指標がAUCである時と大体同じ意味\n",
    "# scikit-learnのmetricsモジュールのroc_auc_scoreを使う\n",
    "from sklearn.metrics import roc_auc_score\n",
    "auc = roc_auc_score(y_true_c, y_prob_c)\n",
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "66e1a466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "# 多クラス分類における評価指標\n",
    "# multi-class accuracy\n",
    "y_true_m = [0, 1, 2, 1, 2, 0]\n",
    "y_pred_m = [0, 1, 2, 0, 1, 1]\n",
    "m_accuracy = accuracy_score(y_true_m, y_pred_m)\n",
    "\n",
    "print(m_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "033fe5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3625557672904274\n"
     ]
    }
   ],
   "source": [
    "# multi-class logloss\n",
    "y_true_m1 = np.array([0, 2, 1, 2, 2])\n",
    "y_pred_m1 = np.array([[0.68, 0.32, 0.00],\n",
    "                     [0.00, 0.00, 1.00],\n",
    "                     [0.60, 0.40, 0.00],\n",
    "                     [0.00, 0.00, 1.00],\n",
    "                     [0.28, 0.12, 0.60]])\n",
    "\n",
    "logloss_m1 = log_loss(y_true_m1, y_pred_m1)\n",
    "print(logloss_m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dd5dfce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5933333333333334 0.5523809523809523 0.6250000000000001\n",
      "0.5933333333333334 0.5523809523809523 0.6250000000000001\n"
     ]
    }
   ],
   "source": [
    "# mean-F1とmacro-F1とmicro-F1\n",
    "# F1-scoreを多クラス分類に拡張したもの\n",
    "# マルチラベルの分類の真の値・予測値は、評価指標の計算上はレコード×クラスの二値の行列とした方が扱いやすい\n",
    "# 真の値 - [[1, 2], [1], [1, 2, 3], [2, 3], [3]]\n",
    "y_true_m2 = np.array([\n",
    "    [1, 1, 0],\n",
    "    [1, 0, 0],\n",
    "    [1, 1, 1],\n",
    "    [0, 1, 1],\n",
    "    [0, 0, 1],\n",
    "])\n",
    "# 予測値- [[1, 3], [2], [1, 3], [3], [3]]\n",
    "y_pred_m2 = np.array([\n",
    "    [1, 0, 1],\n",
    "    [0, 1, 0],\n",
    "    [1, 0, 1],\n",
    "    [0, 0, 1],\n",
    "    [0, 0, 1],\n",
    "])\n",
    "\n",
    "# mean-f1ではレコードごとにf1-scoreを計算して平均をとる\n",
    "mean_f1 = np.mean([f1_score(y_true_m2[i, :], y_pred_m2[i, :]) for i in range(len(y_true_m2))])\n",
    "\n",
    "# macro-f1ではクラスごとにF1-scoreを計算して平均をとる\n",
    "c_class = 3\n",
    "macro_f1 = np.mean([f1_score(y_true_m2[:, c], y_pred_m2[:, c]) for c in range(c_class)])\n",
    "\n",
    "# micro-f1ではレコード×クラスのペアごとにTP/TN/FNを計算し、F1-scoreを求める\n",
    "micro_f1 = f1_score(y_true_m2.reshape(-1), y_pred_m2.reshape(-1))\n",
    "\n",
    "print(mean_f1,  macro_f1, micro_f1)\n",
    "\n",
    "\n",
    "# scikit-learnのメソッドを使うことでも計算できる\n",
    "mean_f1_sk = f1_score(y_true_m2, y_pred_m2, average = 'samples')\n",
    "macro_f1_sk = f1_score(y_true_m2, y_pred_m2, average = 'macro')\n",
    "micro_f1_sk = f1_score(y_true_m2, y_pred_m2, average = 'micro')\n",
    "print(mean_f1_sk, macro_f1_sk, micro_f1_sk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "206e9e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6153846153846154\n",
      "0.6153846153846154\n"
     ]
    }
   ],
   "source": [
    "# quadratic weighted kappa\n",
    "from sklearn.metrics import confusion_matrix, cohen_kappa_score\n",
    "\n",
    "# quadratic weighted kappaを計算する関数\n",
    "def quadratic_weighted_kappa(c_matrix):\n",
    "    numer = 0.0\n",
    "    denom = 0.0\n",
    "    \n",
    "    for i in range(c_matrix.shape[0]):\n",
    "        for j in range(c_matrix.shape[1]):\n",
    "            n = c_matrix.shape[0]\n",
    "            wij = ((i - j) ** 2.0)\n",
    "            oij = c_matrix[i, j]\n",
    "            eij = c_matrix[i, :].sum() * c_matrix[:, j].sum() / c_matrix.sum()\n",
    "            numer += wij * oij\n",
    "            denom += wij * eij\n",
    "            \n",
    "    return 1.0 - numer / denom\n",
    "\n",
    "# y_true_qは真の値のリスト、y_pred_qは予測値のクラスのリスト\n",
    "y_true_q = [1, 2, 3, 4, 3]\n",
    "y_pred_q = [2, 2, 4, 4, 5]\n",
    "\n",
    "# 混合行列(複数クラスの場合の計算)を計算する\n",
    "c_matrix = confusion_matrix(y_true_q, y_pred_q, labels=[1, 2, 3, 4, 5])\n",
    "\n",
    "# quadratic weighted kappaを計算する\n",
    "kappa = quadratic_weighted_kappa(c_matrix)\n",
    "print(kappa)\n",
    "\n",
    "# scikit-learnのメソッドを利用した場合\n",
    "kappa1 = cohen_kappa_score(y_true_q, y_pred_q, weights='quadratic')\n",
    "print(kappa1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f0d0c741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6499999999999999\n"
     ]
    }
   ],
   "source": [
    "# レコメンデーションにおける評価指標\n",
    "# MAP@K\n",
    "# K = 3、レコード数は5個、クラスは4種類とする\n",
    "K = 3\n",
    "\n",
    "# 各レコードの真の値\n",
    "y_true_ma = [[1, 2], [1, 2], [4], [1, 2, 3,4], [3, 4]]\n",
    "\n",
    "# 各レコードに対する予測値ー K = 3なので、通常は各レコードにそれぞれ3個まで順位をつけて予測する\n",
    "y_pred_ma = [[1,2, 4], [4, 1, 2], [1, 4, 3], [1, 2, 3], [1, 2, 4]]\n",
    "\n",
    "# 各レコードごとのaverage precisionを計算する関数\n",
    "def apk(y_i_true, y_i_pred):\n",
    "    # y_predがK以下の長さで、要素が全て異なることが必要\n",
    "    assert (len(y_i_pred) <= K)\n",
    "    assert (len(np.unique(y_i_pred)) == len(y_i_pred))\n",
    "    \n",
    "    sum_precision = 0.0\n",
    "    num_hits = 0.0\n",
    "    \n",
    "    for i, p in enumerate(y_i_pred):\n",
    "        if p in y_i_true:\n",
    "            num_hits += 1\n",
    "            precision = num_hits / (i + 1)\n",
    "            sum_precision += precision\n",
    "            \n",
    "    return sum_precision / min(len(y_i_true), K)\n",
    "\n",
    "\n",
    "# MAP@Kを計算する関数\n",
    "def mapk(y_true, y_pred):\n",
    "    return np.mean([apk(y_i_true, y_i_pred) for y_i_true, y_i_pred in zip(y_true, y_pred)])\n",
    "\n",
    "# MAP@Kを求める\n",
    "print(mapk(y_true_ma, y_pred_ma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ac125801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.5833333333333333\n"
     ]
    }
   ],
   "source": [
    "# 正解の数が同じでも、順位によって差が出る\n",
    "print(apk(y_true_ma[0], y_pred_ma[0]))\n",
    "print(apk(y_true_ma[1], y_pred_ma[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "41039db4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n(参考)通常の方法で学習を行う場合\\nprams = {'silent':1, 'random_state': 71, objective:'binary:logistic'}\\nbst = xgb.train(params, dtrain, num_round, watchlist)\\n\\npred = bst.predict(dvalid)\\nlogloss = log_loss(va_y, pred)\\nprint(logloss)\\n\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# カスタム目的関数とカスタム評価指標\n",
    "# これらを作成するには、使用するライブラリのAPIに沿う形で関数を実装する必要がある\n",
    "# xgboostによる例\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# 特徴量と目的変数をxgboostのデータ構造に変換する\n",
    "# 学習データの特徴量と目的変数がtr_x, tr_y\n",
    "# バリデーションデータの特徴量と目的変数がva_x, va_yとする\n",
    "\"\"\"\n",
    "本来ならtr_xとtr_yとva_xとva_yは得られている\n",
    "dtrain = xgb.DMatrix(tr_x, label = tr_y)\n",
    "dvalid = xgb.DMatrix(va_x, label = va_y)\n",
    "\"\"\"\n",
    "\n",
    "# カスタム目的関数(この場合はloglossであり、xgboostの'binary:logistic'と等価)\n",
    "def logregobj(preds, dtrain):\n",
    "    labels = dtrain.get_label() # 真の値のラベルを取得\n",
    "    preds = 1.0 / (1.0 + np.exp(-preds)) # シグモイド関数\n",
    "    grad = preds - labels # 勾配\n",
    "    hess = preds * (1.0 - preds) # 二階微分値\n",
    "    return grad, hess\n",
    "\n",
    "\n",
    "# カスタム評価指標(この場合は誤答率)\n",
    "def evalerror(preds, dtrain):\n",
    "    labels = dtrain.get_label() # 真の値のラベルを取得\n",
    "    return 'custom-error', float(sum(labels != (preds > 0.0))) / len(labels)\n",
    "\n",
    "\"\"\"\n",
    "# ハイパーパラメータの設定\n",
    "params = {'silent': 1, 'random_state': 71}\n",
    "num_round = 50\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "\n",
    "# モデルの学習実行\n",
    "bst = xgb.train(params, dtrain, num_round, watchlist, obj = logregobj, feval=evalerror)\n",
    "\n",
    "# 目的関数がbinary:logisticを指定した時と違い、\n",
    "# 確率に変換する前の値で予測値が出力されるので変換が必要\n",
    "pred_val = bst.predict(dvalid)\n",
    "pred = 1.0/(1.0 + exp(-pred_val))\n",
    "logloss = log_loss(va_y, pred)\n",
    "print(logloss)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "(参考)通常の方法で学習を行う場合\n",
    "prams = {'silent':1, 'random_state': 71, objective:'binary:logistic'}\n",
    "bst = xgb.train(params, dtrain, num_round, watchlist)\n",
    "\n",
    "pred = bst.predict(dvalid)\n",
    "logloss = log_loss(va_y, pred)\n",
    "print(logloss)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b8cc5985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34257812499999984 0.7559183673469387 0.7570422535211268\n",
      "0.34277343749999983 0.7598457403295548 0.7450980392156863\n",
      "0.31787109374999983 0.7548253676470588 0.7584803256445047\n",
      "0.3234374999999998 0.7545569184913447 0.7588603196664351\n",
      "0.33166503906249983\n"
     ]
    }
   ],
   "source": [
    "# 閾値の最適化をout-of-foldで行う\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# サンプルデータ生成の準備\n",
    "rand = np.random.RandomState(seed = 71)\n",
    "train_y_prob = np.linspace(0, 1.0, 10000)\n",
    "\n",
    "# 真の値と予測値が以下のtrain_y, train_pred_probとする\n",
    "# np.clipはNumPy配列ndarrayの要素の値を任意の範囲内に収めるクリッピング処理\n",
    "# を行うにはnp.clip()またはndarrayのclip()メソッドを使う。\n",
    "# 引数に最小値と最大値を指定すると、その範囲外の値は最小値または最大値に置き換えられる。\n",
    "# https://note.nkmk.me/python-numpy-clip/\n",
    "train_y = pd.Series(rand.uniform(0.0, 1.0, train_y_prob.size) < train_y_prob)\n",
    "train_pred_prob = np.clip(train_y_prob * np.exp(rand.standard_normal(train_y_prob.shape)*0.3), 0.0, 1.0)\n",
    "\n",
    "# クロスバリデーションの枠組みで閾値を求める\n",
    "thresholds = []\n",
    "scores_tr =[]\n",
    "scores_va = []\n",
    "\n",
    "kf = KFold(n_splits=4, random_state=71, shuffle=True)\n",
    "for i, (tr_idx, va_idx) in enumerate(kf.split(train_pred_prob)):\n",
    "    tr_pred_prob, va_pred_prob = train_pred_prob[tr_idx], train_pred_prob[va_idx]\n",
    "    tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]\n",
    "    \n",
    "    # 最適化の目的関数を設定\n",
    "    def f1_opt(x):\n",
    "        # マイナスをつけているのは最適化ライブラリは基本的に最小化する方向性の物が多く、今回もminimizeを利用するから\n",
    "        return -f1_score(tr_y, tr_pred_prob >= x)\n",
    "    \n",
    "    # 学習データで閾値の最適化を行い、バリデーションデータで評価を行う\n",
    "    result = minimize(f1_opt, x0 = np.array([0.5]), method = 'Nelder-Mead')\n",
    "    threshold = result['x'].item()\n",
    "    score_tr = f1_score(tr_y, tr_pred_prob >= threshold)\n",
    "    score_va = f1_score(va_y, va_pred_prob >= threshold)\n",
    "    print(threshold, score_tr, score_va)\n",
    "    \n",
    "    thresholds.append(threshold)\n",
    "    scores_tr.append(score_tr)\n",
    "    scores_va.append(score_va)\n",
    "    \n",
    "# 各foldの閾値の平均をテストデータには適応する\n",
    "threshold_test = np.mean(thresholds)\n",
    "print(threshold_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09827b07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
